# vLLM

![alt text](image-1.png)

## References

- [vLLM is a fast and easy-to-use library for LLM inference and serving.](https://docs.vllm.ai/en/latest/index.html)

- [Serve Gemma open models using GPUs on GKE with vLLM ](https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm)