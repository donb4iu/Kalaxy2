# Triton and TensorRT LLM

![alt text](image-3.png)

## References

- [Serve Gemma open models using GPUs on GKE with Triton and TensorRT-LLM](https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-tensortllm)

- [Welcome to TensorRT-LLMâ€™s Documentation!](https://nvidia.github.io/TensorRT-LLM/)

- [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)